{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat -cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import random\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "# import faiss\n",
    "# import faiss.contrib.torch_utils\n",
    "\n",
    "from image import imread, imwrite, imshow\n",
    "from fold import fold2d, unfold2d\n",
    "from nnlookup import nn_lookup2d, nn_lookup_soft2d, l2_dist\n",
    "from utils import view_as_2d, view_2d_as\n",
    "from Resizer import Resizer\n",
    "\n",
    "# from image import to_numpy, np2pt, pt2np\n",
    "# from nnlookup import nn_lookup, nn_lookup_soft, nn_lookup_soft2d\n",
    "# from nnlookup import l2_dist, inner_prod_dist\n",
    "# from utils import view_3d_as_6d, view_6d_as_3d\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_pyramid(image, depth, ratio, verbose=False):\n",
    "    device = image.device\n",
    "    max_layer = depth\n",
    "    pyramid = []\n",
    "    ratio = _pair(ratio)\n",
    "    curr = image\n",
    "    pyramid.append(curr)\n",
    "    for j in range(max_layer):\n",
    "        if verbose:\n",
    "            print(curr.shape)\n",
    "            imshow(curr)\n",
    "        shape = [1, image.shape[1], ceil(image.shape[2] * (ratio[0])**(j+1)), ceil(image.shape[3] * (ratio[1])**(j+1))]\n",
    "        resizer = Resizer(curr.shape, ratio, shape).to(device=device)\n",
    "        curr = resizer(curr)\n",
    "        pyramid.append(curr)\n",
    "    return pyramid\n",
    "\n",
    "\n",
    "def get_pyramid_from_file(fname, *args, device=None, **kwargs):\n",
    "    image = imread(file, pt=True).to(device=device)            \n",
    "    return get_pyramids_aux(image, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_DTYPE = torch.float32\n",
    "SOFTMIN_DTYPE = torch.float32\n",
    "\n",
    "def extract_kvs(keys, values, patch_size, use_padding):\n",
    "    if isinstance(keys, torch.Tensor):\n",
    "        keys = [keys]\n",
    "    if isinstance(values, torch.Tensor):\n",
    "        values = [values]\n",
    "    assert len(keys) == len(values)\n",
    "    assert all(k.shape == v.shape for k, v in zip(keys, values))\n",
    "    key = torch.cat([view_as_2d(unfold2d(k, patch_size, use_padding=use_padding))[0] for k in keys], dim=0).contiguous()\n",
    "    value = torch.cat([view_as_2d(unfold2d(v, patch_size, use_padding=use_padding))[0] for v in values], dim=0).contiguous()\n",
    "#     return key.to(dtype=NN_DTYPE), value.to(dtype=NN_DTYPE)\n",
    "    return key, value\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_patch_weight_min_dist(query, key, dist_fn, alpha, batch_size):\n",
    "    kbatch = (batch_size + query.shape[0] - 1) // query.shape[0]\n",
    "    min_dists = []\n",
    "    for j in range((key.shape[0] + kbatch - 1) // kbatch):\n",
    "        idx = nn_lookup2d(key[j*kbatch:(j+1)*kbatch], query, dist_fn=dist_fn)\n",
    "        match_query = torch.index_select(query, 0, idx)\n",
    "        min_dists.append((key[j*kbatch:(j+1)*kbatch] - match_query).pow(2).mean(1))\n",
    "    min_dist = torch.cat(min_dists, dim=0)\n",
    "    weight = 1 / (alpha + min_dist)\n",
    "    return weight\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_patch_nn(query, key, value, patch_size, dist_fn, bidirectional=False, alpha=1., temperature=None, reduce='mean', use_padding=False, batch_size=2**28):\n",
    "    # reshape input\n",
    "    # dtype = query.dtype  # XXX\n",
    "    query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "    query, size, ndim = view_as_2d(query)\n",
    "    # query = query.to(dtype=NN_DTYPE)\n",
    "    # print(query.shape, key.shape)\n",
    "    # perform nn-search\n",
    "    idxs = []\n",
    "    qbatch = (batch_size + key.shape[0] - 1) // key.shape[0]\n",
    "    kbatch = (batch_size + query.shape[0] - 1) // query.shape[0]\n",
    "    if bidirectional:\n",
    "        weight = torch_patch_weight_min_dist(query, key, dist_fn, alpha, batch_size).view(1, -1)\n",
    "    else:\n",
    "        weight = None\n",
    "    for i in range((query.shape[0] + qbatch - 1) // qbatch):\n",
    "        if temperature is None:\n",
    "            idxs.append(nn_lookup2d(query[i*qbatch:(i+1)*qbatch], key, weight=weight, dist_fn=dist_fn))\n",
    "        else:\n",
    "            idxs.append(nn_lookup_soft2d(query[i*qbatch:(i+1)*qbatch], key, weight=weight, dist_fn=dist_fn, temperature=temperature, dtype=SOFTMIN_DTYPE))\n",
    "    idx = torch.cat(idxs, dim=0)\n",
    "    result = torch.index_select(value, 0, idx)\n",
    "\n",
    "    # reshape output\n",
    "    result = view_2d_as(result, size, ndim)\n",
    "    # result = result.to(dtype=dtype)\n",
    "    output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "    return output\n",
    "\n",
    "\n",
    "class TorchPatchNN(nn.Module):\n",
    "    def __init__(self, patch_size, dist_fn, bidirectional=False, alpha=1., temperature=None, reduce='mean', use_padding=False, batch_size=2**28):\n",
    "        super().__init__()\n",
    "        self._patch_size = _pair(patch_size)\n",
    "        self._dist_fn = dist_fn\n",
    "        self._bidirectional = bidirectional\n",
    "        self._alpha = alpha\n",
    "        self._temperature = temperature\n",
    "        self._reduce = reduce\n",
    "        self._use_padding = use_padding\n",
    "        self._batch_size = batch_size\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        return torch_patch_nn(\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value,\n",
    "            patch_size=self._patch_size,\n",
    "            dist_fn=self._dist_fn,\n",
    "            bidirectional=self._bidirectional,\n",
    "            alpha=self._alpha,\n",
    "            temperature=self._temperature,\n",
    "            reduce=self._reduce,\n",
    "            use_padding=self._use_padding,\n",
    "            batch_size=self._batch_size)\n",
    "    \n",
    "    def extract_kvs(self, keys, values):\n",
    "        key, value = extract_kvs(keys, values, self._patch_size, use_padding=self._use_padding)\n",
    "        return key, value\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}(patch_size={}, dist_fn={}, bidirectional={}, alpha={}, reduce=\"{}\", use_padding={}, batch_size={})'.format(\n",
    "            self.__class__.__name__,\n",
    "            self._patch_size,\n",
    "            self._dist_fn.__name__,\n",
    "            self._bidirectional,\n",
    "            self._alpha,\n",
    "            self._reduce,\n",
    "            self._use_padding,\n",
    "            self._batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def new_image_generation(pnn, src_pyramid, dst_pyramid, ratio=4/3, noise_std=0.75, noise_decay=None, num_iters=10, top_level=9, verbose=False):\n",
    "    to = {'device': src_pyramid[0].device, 'dtype': src_pyramid[0].dtype}\n",
    "    start = time.time()\n",
    "    new_im = dst_pyramid[top_level]\n",
    "    new_im = new_im + noise_std * torch.randn_like(new_im)\n",
    "    if noise_decay is None:\n",
    "        noise_decay = 0.\n",
    "    for l in range(top_level, -1, -1):\n",
    "        start = time.time()\n",
    "        resizer = Resizer(src_pyramid[l + 1].shape, ratio, src_pyramid[l].shape).to(**to)\n",
    "        if l == top_level:\n",
    "            curr = src_pyramid[l]\n",
    "            prev = src_pyramid[l]\n",
    "        else:\n",
    "            curr = src_pyramid[l]\n",
    "            prev = resizer(src_pyramid[l + 1])\n",
    "        # key, value = extract_kvs(curr, prev, pnn._patch_size, use_padding=pnn._use_padding)\n",
    "        key_index, value = pnn.extract_kvs(keys=prev, values=curr) # TODO: change keys to prev! # create_index_l2(key)\n",
    "        for k in range(num_iters if l != top_level else 1):\n",
    "            # new_im = pnn(new_im, keys=prev, values=curr)\n",
    "            start = time.time()\n",
    "            new_im = pnn(new_im, key_index, value)\n",
    "            # print(l, k, '%.2fms' % (1000 * (time.time() - start),))\n",
    "#         print(new_im.shape)\n",
    "#         imshow(new_im)\n",
    "        if l > 0:\n",
    "            resizer = Resizer(dst_pyramid[l].shape, ratio, dst_pyramid[l - 1].shape).to(**to)\n",
    "            new_im = resizer(new_im)\n",
    "            new_im = new_im + (noise_std * noise_decay ** (top_level + 1 - l)) * torch.randn_like(new_im)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Total time: %.2f[s]' % (time.time() - start,))\n",
    "        imshow(new_im)\n",
    "    \n",
    "    return new_im\n",
    "\n",
    "@torch.no_grad()\n",
    "def structural_analogy(pnn, path_a, path_b, num_iters=10, top_level=9, verbose=False):\n",
    "#     to = {'device': src_pyramid[0].device, 'dtype': src_pyramid[0].dtype}\n",
    "    start = time.time()\n",
    "    pyr = get_pyramid(imread(path_a).cuda(), depth=20, ratio=0.8, verbose=False)\n",
    "    pyr2 = get_pyramid(imread(path_b).cuda(), depth=20, ratio=0.8, verbose=False)\n",
    "    new_im = pyr2[0].clone()\n",
    "    for i in range(2):\n",
    "        new_pyr = get_pyramid(new_im, depth=20, ratio=0.8, verbose=False)\n",
    "        new_im = new_pyr[top_level].clone()\n",
    "        for l in range(top_level, -1, -1):\n",
    "            resizer = Resizer(pyr[l + 1].shape, 5/4, pyr[l].shape).cuda()\n",
    "            curr = pyr[l]\n",
    "            prev = resizer(pyr[l + 1])\n",
    "            # key, value = extract_kvs(curr, prev, pnn._patch_size, use_padding=pnn._use_padding)\n",
    "            key_index, value = pnn.extract_kvs(keys=[prev,curr], values=[curr,curr])  # create_index_l2(key)\n",
    "            for k in range(num_iters):\n",
    "                start = time.time()\n",
    "                new_im = pnn(new_im, key_index, value)\n",
    "            imshow(torch.cat([new_im, pyr2[l]], dim=-1))\n",
    "            if l > 0:\n",
    "                resizer = Resizer(new_pyr[l].shape, 5/4, new_pyr[l - 1].shape).cuda()\n",
    "                new_im = resizer(new_im)    \n",
    "    if verbose:\n",
    "        print('Total time: %.2f[s]' % (time.time() - start,))\n",
    "        imshow(new_im)\n",
    "    \n",
    "    return new_im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = (7, 7)\n",
    "# im = gt.to(device=device)\n",
    "# imshow(im)\n",
    "\n",
    "# # pyramid = [level.to(device=device) for level in pyramid]\n",
    "# for i, level in enumerate(pyramid):\n",
    "#     print(i, level.shape)\n",
    "#     imshow(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = imread('/home/nivg/data/balloons.png', pt=True).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (7, 7)\n",
    "# pnn = FaissPatchNNL2(patch_size=patch_size, reduce='weighted_mean', use_padding=False)\n",
    "pnn = TorchPatchNN(patch_size=patch_size, dist_fn=l2_dist, batch_size=2**28, bidirectional=False, alpha=1, reduce='weighted_mean')\n",
    "pnn = pnn.to(device=device)\n",
    "print(pnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = gt.to(device=device)\n",
    "dtype = torch.float32\n",
    "orig_ratio = 3 / 4\n",
    "noise_std = 0.75\n",
    "noise_decay = 0.0\n",
    "depth = 15\n",
    "pyr = get_pyramid(im, depth=depth, ratio=orig_ratio, verbose=False)\n",
    "pyr = [lvl.to(dtype=dtype) for lvl in pyr]\n",
    "pyr = pyr[0:]\n",
    "imshow(pyr[0])\n",
    "pnn._temperature = None\n",
    "SOFTMIN_DTPYE = torch.float64\n",
    "\n",
    "for ratio in [(1, 1)]:#, (1, 4/5), (1, 3/4), (1, 1/3), (1, 5/4), (1, 2)]:\n",
    "    rsizer = Resizer(pyr[0].shape, ratio).to(device=device)\n",
    "    dst_pyr = get_pyramid(rsizer(pyr[0]), depth=len(pyr) + 1, ratio=orig_ratio, verbose=False)\n",
    "    dst_pyr = [lvl.to(dtype=dtype) for lvl in dst_pyr]\n",
    "    for k in range(10):\n",
    "        start = time.time()\n",
    "#         out = new_image_generation_old(pyr, dst_pyr, patch_size)\n",
    "        out = new_image_generation(pnn, pyr, dst_pyr, top_level=9, ratio=1/orig_ratio, noise_std=noise_std, noise_decay=noise_decay, verbose=False)\n",
    "        print('ratio=%s' % (ratio,), 'k=%d' % (k,), 'time=%.2fms' % (1000 * (time.time() - start),))\n",
    "        imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4_real_a, 4_real_b - 6, 1e-2\n",
    "# 4_real_b, 4_real_a - 6, 5e-2\n",
    "\n",
    "# 10_real_a, 10_real_b - 6, 5e-3\n",
    "# 10_real_b, 10_real_a - 7, 5e-3\n",
    "\n",
    "# factory_real_a, factory_real_b - 8, 5e-2\n",
    "# factory_real_b, factory_real_a - 8, 5e-3\n",
    "\n",
    "# flowers_real_a, flowers_real_b - 7, 5e-3\n",
    "# flowers_real_b, flowers_real_a - 7, 5e-3\n",
    "\n",
    "# flowers2_real_b, flowers2_real_a - 11, 5e-3\n",
    "# flowers2_real_a, flowers2_real_b - 10, 5e-3\n",
    "\n",
    "# knit_real_a, knit_real_b - 3/4, 5e-4\n",
    "# knit_real_b, knit_real_a - 6, 5e-3\n",
    "\n",
    "# perspective_real_a, perspective_real_b - 4, 5e-3\n",
    "# perspective_real_b, perspective_real_a - 8, 5e-2\n",
    "\n",
    "# snow_real_a (1), snow_real_b (2) - 5, 5e-3\n",
    "# snow_real_b (2), snow_real_a (1) - 4, 5e-2\n",
    "\n",
    "# marble_real_a, marble_real_b - 8, 5e-3\n",
    "# marble_real_b, marble_real_a - 9, 5e-4\n",
    "\n",
    "# 0_real_a, 0_real_b - 8, 5e-4\n",
    "# 0_real_b, 0_real_a - 8, 5e-4\n",
    "\n",
    "# sky_real_a, sky_real_b - 6, 5e-4\n",
    "# sky_real_b, sky_real_a - 6, 5e-4\n",
    "\n",
    "# oranges_real_a, oranges_real_b - 6, 5e-4\n",
    "# oranges_real_b, oranges_real_a - 7, 5e-4\n",
    "\n",
    "# tennis_real_a (1), tennis_real_b - 6, 5e-5\n",
    "# tennis_real_b, tennis_real_a (1) - 9, 5e-3\n",
    "\n",
    "# dirtroads_real_a, dirtroads_real_b - 7, 5e-3\n",
    "# dirtroads_real_b, dirtroads_real_a - 8, 5e-4\n",
    "\n",
    "# snow2ice_real_a, snow2ice_real_b - 10, 5e-3\n",
    "# snow2ice_real_b, snow2ice_real_a - 11, 5e-2\n",
    "\n",
    "# 22_real_a, 22_real_b - 8, 5e-4 / 10,5e-4\n",
    "# 22_real_b, 22_real_a - 7, 5e-4\n",
    "\n",
    "# 206, 207 - 10, 5e-3\n",
    "# 207, 206 - 11, 5e-1\n",
    "# 204, 205 - 4, 5e-3\n",
    "# 205, 204 - 4, 5e-3\n",
    "# 202, 203 - 2, 5e-4\n",
    "# 203, 202 - 2, 5e-4\n",
    "# 208_resized, 209 - 10, 5e-3\n",
    "# 209, 208_resized - 6, 5e-3\n",
    "# 10, 11 - 6, 5e-3\n",
    "# 108, 109 - 4, 5e-3\n",
    "# 109, 108 - 6, 5e-3\n",
    "# trees_paint, trees - 3, 5e-2\n",
    "# cows_paint, cows - 3, 5e-4\n",
    "# 309_resized, 308 - 6, 5e-3\n",
    "\n",
    "\n",
    "pnn = TorchPatchNN(patch_size=7, dist_fn=l2_dist, batch_size=2**28, bidirectional=True, alpha=5e-3, reduce='mean')\n",
    "pnn = pnn.to(device=device)\n",
    "\n",
    "path_a = '/home/nivg/data/DropGAN/Analogies/for_assaf/marble_real_b.jpg'\n",
    "path_b = '/home/nivg/data/DropGAN/Analogies/for_assaf/marble_real_a.jpg'\n",
    "\n",
    "structural_analogy(pnn, path_a, path_b, num_iters=10, top_level=8, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def torch_patch_weight_min_dist(query, key, dist_fn, batch_size, alpha=1):\n",
    "#     kbatch = (batch_size + query.shape[0] - 1) // query.shape[0]\n",
    "#     min_dists = []\n",
    "#     for j in range((key.shape[0] + kbatch - 1) // kbatch):\n",
    "#         idx = nn_lookup2d(key[j*kbatch:(j+1)*kbatch], query, dist_fn=dist_fn)\n",
    "#         match_query = torch.index_select(query, 0, idx)\n",
    "#         min_dists.append((key[j*kbatch:(j+1)*kbatch] - match_query).pow(2).sum(1))\n",
    "#     min_dist = torch.cat(min_dists, dim=0)\n",
    "#     weight = 1 / (alpha + min_dist)\n",
    "#     return weight\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def torch_patch_nn(query, key, value, patch_size, dist_fn, bidirectional=False, reduce='mean', use_padding=False, batch_size=2**28):\n",
    "#     # reshape input\n",
    "#     query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "#     query, size, ndim = view_as_2d(query)\n",
    "    \n",
    "#     # perform nn-search\n",
    "#     idxs = []\n",
    "#     qbatch = (batch_size + key.shape[0] - 1) // key.shape[0]\n",
    "#     kbatch = (batch_size + query.shape[0] - 1) // query.shape[0]\n",
    "#     if bidirectional:\n",
    "#         weight = torch_patch_weight_min_dist(query, key, dist_fn, batch_size).view(1, -1)\n",
    "#     else:\n",
    "#         weight = None\n",
    "#     for i in range((query.shape[0] + qbatch - 1) // qbatch):\n",
    "#         idxs.append(nn_lookup2d(query[i*qbatch:(i+1)*qbatch], key, weight=weight, dist_fn=dist_fn))\n",
    "#     idx = torch.cat(idxs, dim=0)\n",
    "#     result = torch.index_select(value, 0, idx)\n",
    "    \n",
    "#     # reshape output\n",
    "#     result = view_2d_as(result, size, ndim)\n",
    "#     output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio=(1, 1) k=0 time=8596.04ms\n",
    "# ratio=(1, 1) k=1 time=8577.48ms\n",
    "# ratio=(1, 0.8) k=0 time=7105.97ms\n",
    "# ratio=(1, 0.8) k=1 time=7127.98ms\n",
    "# ratio=(1, 0.75) k=0 time=6812.93ms\n",
    "# ratio=(1, 0.75) k=1 time=6795.47ms\n",
    "# ratio=(1, 0.3333333333333333) k=0 time=3733.47ms\n",
    "# ratio=(1, 0.3333333333333333) k=1 time=3683.60ms\n",
    "# ratio=(1, 1.25) k=0 time=10514.17ms\n",
    "# ratio=(1, 1.25) k=1 time=10468.91ms\n",
    "# ratio=(1, 2) k=0 time=16047.43ms\n",
    "# ratio=(1, 2) k=1 time=16064.11ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD FP16\n",
    "# ratio=(1, 1) k=0 time=1168.27ms\n",
    "# ratio=(1, 1) k=1 time=1215.02ms\n",
    "# ratio=(1, 0.8) k=0 time=952.06ms\n",
    "# ratio=(1, 0.8) k=1 time=1015.07ms\n",
    "# ratio=(1, 0.75) k=0 time=925.23ms\n",
    "# ratio=(1, 0.75) k=1 time=983.23ms\n",
    "# ratio=(1, 0.3333333333333333) k=0 time=478.62ms\n",
    "# ratio=(1, 0.3333333333333333) k=1 time=511.96ms\n",
    "# ratio=(1, 1.25) k=0 time=1365.95ms\n",
    "# ratio=(1, 1.25) k=1 time=1461.49ms\n",
    "# ratio=(1, 2) k=0 time=2097.24ms\n",
    "# ratio=(1, 2) k=1 time=2239.94ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_faiss_gpu_config(device, dtype):\n",
    "#     res = faiss.StandardGpuResources()\n",
    "#     cfg = faiss.GpuIndexFlatConfig()\n",
    "#     cfg.useFloat16 = (dtype == torch.float16)\n",
    "#     cfg.device = device.index\n",
    "#     return res, cfg\n",
    "\n",
    "# def faiss_create_index_l2(key):\n",
    "#     assert key.dim() == 2\n",
    "#     d = key.shape[1]\n",
    "#     if key.device.type == 'cpu':\n",
    "#         index = faiss.IndexFlatL2(d)\n",
    "#     elif key.device.type == 'cuda':\n",
    "#         res, cfg = _get_faiss_gpu_config(key.device, key.dtype)\n",
    "#         index = faiss.GpuIndexFlatL2(res, d, cfg)\n",
    "#     else:\n",
    "#         raise ValueError('unsupported device: {}'.format(key.device))        \n",
    "#     index.add(key)\n",
    "#     return index\n",
    "\n",
    "# # @torch.no_grad()\n",
    "# # def patch_nn_l2(query, keys, values, patch_size, reduce='mean', use_padding=False):\n",
    "# #     # reshape input\n",
    "# #     query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "# #     query, size, ndim = view_as_2d(query)\n",
    "# #     key, value = extract_kvs(keys, values, patch_size, use_padding=use_padding)\n",
    "    \n",
    "# #     # perform nn-search\n",
    "# #     key_index = create_index_l2(key)\n",
    "# #     idx = key_index.search(query.contiguous(), 1)[1].squeeze(-1)\n",
    "# #     result = torch.index_select(value, 0, idx)\n",
    "    \n",
    "# #     # reshape output\n",
    "# #     result = view_2d_as(result, size, ndim)\n",
    "# #     output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "# #     return output\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def faiss_patch_nn_l2(query, key_index, value, patch_size, reduce='mean', use_padding=False):\n",
    "#     # reshape input\n",
    "#     query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "#     query, size, ndim = view_as_2d(query)\n",
    "#     # key, value = extract_kvs(keys, values, patch_size, use_padding=use_padding)\n",
    "    \n",
    "#     # perform nn-search\n",
    "#     # key_index = create_index_l2(key)\n",
    "#     idx = key_index.search(query.contiguous(), 1)[1].squeeze(-1)\n",
    "#     result = torch.index_select(value, 0, idx)\n",
    "    \n",
    "#     # reshape output\n",
    "#     result = view_2d_as(result, size, ndim)\n",
    "#     output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "#     return output\n",
    "\n",
    "# class FaissPatchNNL2(nn.Module):\n",
    "#     def __init__(self, patch_size, reduce='mean', use_padding=False):\n",
    "#         super().__init__()\n",
    "#         self._patch_size = _pair(patch_size)\n",
    "#         self._reduce = reduce\n",
    "#         self._use_padding = use_padding\n",
    "    \n",
    "#     def forward(self, query, key_index, value):\n",
    "#         return faiss_patch_nn_l2(\n",
    "#             query,\n",
    "#             key_index,\n",
    "#             value,\n",
    "#             patch_size=self._patch_size,\n",
    "#             reduce=self._reduce,\n",
    "#             use_padding=self._use_padding)\n",
    "    \n",
    "#     def extract_kvs(self, keys, values):\n",
    "#         key, value = extract_kvs(keys, values, self._patch_size, use_padding=self._use_padding)\n",
    "#         key_index = faiss_create_index_l2(key)\n",
    "#         return key_index, value\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return '{0}(patch_size={1}, reduce=\"{2}\", use_padding={3})'.format(\n",
    "#             self.__class__.__name__, self._patch_size, self._reduce, self._use_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def torch_patch_nn_l2(query, key, value, patch_size, batch_size=16777216, reduce='mean', use_padding=False):\n",
    "#     # reshape input\n",
    "#     query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "#     query, size, ndim = view_as_2d(query)\n",
    "    \n",
    "#     # perform nn-search\n",
    "#     idxs = []\n",
    "#     qbatch = (batch_size + key.shape[0] - 1) // key.shape[0]\n",
    "#     kbatch = (batch_size + query.shape[0] - 1) // query.shape[0]\n",
    "#     for i in range((query.shape[0] + qbatch - 1) // qbatch):\n",
    "#         idxs.append(nn_lookup2d(query[i*qbatch:(i+1)*qbatch], key))\n",
    "#     idx = torch.cat(idxs, dim=0)\n",
    "#     result = torch.index_select(value, 0, idx)\n",
    "    \n",
    "#     # reshape output\n",
    "#     result = view_2d_as(result, size, ndim)\n",
    "#     output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# class TorchPatchNNL2(nn.Module):\n",
    "#     def __init__(self, patch_size, batch_size=16777216, reduce='mean', use_padding=False):\n",
    "#         super().__init__()\n",
    "#         self._patch_size = _pair(patch_size)\n",
    "#         self._reduce = reduce\n",
    "#         self._use_padding = use_padding\n",
    "#         self._batch_size = batch_size\n",
    "    \n",
    "#     def forward(self, query, key, value):\n",
    "#         return torch_patch_nn_l2(\n",
    "#             query=query,\n",
    "#             key=key,\n",
    "#             value=value,\n",
    "#             patch_size=self._patch_size,\n",
    "#             batch_size=self._batch_size,\n",
    "#             reduce=self._reduce,\n",
    "#             use_padding=self._use_padding)\n",
    "    \n",
    "#     def extract_kvs(self, keys, values):\n",
    "#         key, value = extract_kvs(keys, values, self._patch_size, use_padding=self._use_padding)\n",
    "#         return key, value\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return '{0}(patch_size={1}, reduce=\"{2}\", use_padding={3}, batch_size={4})'.format(\n",
    "#             self.__class__.__name__, self._patch_size, self._reduce, self._use_padding, self._batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FP16\n",
    "# ratio=(1, 1) k=0 time=1677.75ms\n",
    "# ratio=(1, 1) k=1 time=1769.13ms\n",
    "# ratio=(1, 0.8) k=0 time=1401.01ms\n",
    "# ratio=(1, 0.8) k=1 time=1493.48ms\n",
    "# ratio=(1, 0.75) k=0 time=1350.04ms\n",
    "# ratio=(1, 0.75) k=1 time=1436.66ms\n",
    "# ratio=(1, 0.3333333333333333) k=0 time=637.96ms\n",
    "# ratio=(1, 0.3333333333333333) k=1 time=688.41ms\n",
    "# ratio=(1, 1.25) k=0 time=1993.33ms\n",
    "# ratio=(1, 1.25) k=1 time=2133.84ms\n",
    "# ratio=(1, 2) k=0 time=3086.48ms\n",
    "# ratio=(1, 2) k=1 time=3301.28ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyr = get_pyramid('/home/nivg/data/mountains.jpg', None, 0, 15, 0.75, print_=True) \n",
    "# pyr=pyr[3:]\n",
    "# for ratio in ([[1,1],[1,4/5],[1,3/4],[1,1/3],[1,5/4],[1,2]]):\n",
    "#     rsizer = Resizer.Resizer(pyr[0].shape, ratio).cuda()\n",
    "#     dst_pyr = get_pyramids_aux(rsizer(pyr[0]), num_layers=15, ratio=3/4, print_=False)\n",
    "#     for k in range(2):\n",
    "#         im = new_image_generation(pyr, dst_pyr)\n",
    "# #         imwrite('/home/nivg/data/DropGAN/image_generation/' + IM_NAME + '_' + str(ratio[0]) + '_' + str(ratio[1]) +  '_im' + str(k) + '.png', im.squeeze(0).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patch2im(input, w, h, patch_size=7):\n",
    "#     patch_size = _pair(patch_size)\n",
    "#     out = input.transpose(1,2)\n",
    "#     normalize = torch.ones_like(out)\n",
    "#     normalize = normalize.transpose(1,2)\n",
    "#     fold = F.fold(\n",
    "#         input, \n",
    "#         output_size=(w, h), \n",
    "#         kernel_size=patch_size\n",
    "#     )\n",
    "\n",
    "#     norm = F.fold(\n",
    "#         normalize,\n",
    "#         output_size=(w, h),\n",
    "#         kernel_size=patch_size\n",
    "#     )\n",
    "\n",
    "#     return fold / norm\n",
    "\n",
    "\n",
    "# def _calc_dist_l2(X, Y):\n",
    "#     Y = Y.transpose(0, 1)\n",
    "#     X2 = X.pow(2).sum(1, keepdim=True)\n",
    "#     Y2 = Y.pow(2).sum(0, keepdim=True)\n",
    "#     XY = X @ Y\n",
    "#     return X2 - (2 * XY) + Y2\n",
    "\n",
    "# DIV = 100\n",
    "# import math\n",
    "\n",
    "# def build_image(input_img, index_imgs, ref_imgs, patch_size=7):\n",
    "#     patch_size = _pair(patch_size)\n",
    "#     unfold = nn.Unfold(kernel_size=patch_size)\n",
    "#     in_patches = unfold(input_img)\n",
    "#     for i, (ind_img, ref_img) in enumerate(zip(index_imgs, ref_imgs)):\n",
    "#         if i == 0:\n",
    "#             index_patches = unfold(ind_img)\n",
    "#             ref_patches = unfold(ref_img)\n",
    "#         else:\n",
    "#             index_patches = torch.cat([index_patches, unfold(ind_img)],dim=-1)\n",
    "#             ref_patches = torch.cat([ref_patches, unfold(ref_img)], dim=-1)\n",
    "            \n",
    "#     if in_patches.shape[-1]*ref_patches.shape[-1] > 14000**2:\n",
    "#         for j in range(DIV):\n",
    "#             start_patch = j*(math.ceil(in_patches.shape[-1]/DIV))\n",
    "#             end_patch = min((j+1)*(math.ceil(in_patches.shape[-1]/DIV)), in_patches.shape[-1])\n",
    "#             dist_mat = _calc_dist_l2(in_patches[:,:,start_patch:end_patch].permute(0,2,1).squeeze(0), index_patches.squeeze(0).permute(1,0))\n",
    "# #                 print(start_patch, end_patch, dist_mat.shape)\n",
    "#             if j == 0:\n",
    "#                 ind = dist_mat.argmin(1)\n",
    "#             elif start_patch < end_patch:\n",
    "#                 ind = torch.cat([ind, dist_mat.argmin(1)])\n",
    "\n",
    "#     else:\n",
    "#         dist_mat = _calc_dist_l2(in_patches.permute(0,2,1).squeeze(0), index_patches.squeeze(0).permute(1,0))\n",
    "#         ind = dist_mat.argmin(1)\n",
    "#     out_patches = F.embedding(ind, ref_patches.squeeze(0).permute(1,0))\n",
    "#     return patch2im(out_patches.unsqueeze(0).permute(0,2,1), input_img.shape[-2], input_img.shape[-1], patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def new_image_generation_old(src_pyramid, dst_pyramid, patch_size, ratio=4/3, noise_std=0.75, num_iters=10, top_level=9, verbose=False):\n",
    "#     to = {'device': src_pyramid[0].device, 'dtype': src_pyramid[0].dtype}\n",
    "#     patch_size = _pair(patch_size)\n",
    "#     start = time.time()\n",
    "#     new_im = dst_pyramid[top_level] + torch.randn_like(dst_pyramid[top_level]) * noise_std\n",
    "#     for l in range(top_level, -1, -1):\n",
    "#         start = time.time()\n",
    "#         resizer = Resizer(src_pyramid[l + 1].shape, ratio, src_pyramid[l].shape).to(**to)\n",
    "#         if l == top_level:\n",
    "#             curr = [src_pyramid[l]]\n",
    "#             prev = [src_pyramid[l]]\n",
    "#         else:\n",
    "#             curr = [src_pyramid[l]]\n",
    "#             prev = [resizer(src_pyramid[l + 1])]\n",
    "#         for k in range(num_iters if l != top_level else 1):\n",
    "#             # new_im = pnn(new_im, keys=prev, values=curr)\n",
    "#             start = time.time()\n",
    "#             new_im = build_image(new_im, prev, curr, patch_size=patch_size)\n",
    "#             # print(l, k, '%.2fms' % (1000 * (time.time() - start),))\n",
    "#         if l > 0:\n",
    "#             resizer = Resizer(dst_pyramid[l].shape, ratio, dst_pyramid[l - 1].shape).to(**to)\n",
    "#             new_im = resizer(new_im)\n",
    "    \n",
    "#     if verbose:\n",
    "#         print('Total time: %.2f[s]' % (time.time() - start,))\n",
    "#         imshow(new_im)\n",
    "    \n",
    "#     return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = (7, 7)\n",
    "# im = imread('/home/nivg/data/mountains.jpg', pt=True).to(device=device)\n",
    "# # im = torch.cat([im, im], dim=-1)\n",
    "# # im = torch.cat([im, im], dim=-2)\n",
    "\n",
    "# pnn = PatchNNL2(patch_size=patch_size, reduce='weighted_mean', use_padding=False)  # , faiss_device=faiss_gpu_device)\n",
    "# print(pnn)\n",
    "\n",
    "# start = torch.cuda.Event(enable_timing=True)\n",
    "# end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "# start.record()\n",
    "# with torch.no_grad():\n",
    "#     out = pnn(query=im, keys=[im], values=[im])\n",
    "# end.record()\n",
    "# torch.cuda.synchronize()\n",
    "# print('time = %.2fms' % start.elapsed_time(end))\n",
    "# # imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = gt.to(device=device)\n",
    "# pyr = get_pyramid(im, depth=15, ratio=3/4, verbose=False)\n",
    "# pyr = pyr[3:]\n",
    "# for ratio in [(1, 1), (1, 4/5), (1, 3/4), (1, 1/3), (1, 5/4), (1, 2)]:\n",
    "#     rsizer = Resizer(pyr[0].shape, ratio).to(device=device)\n",
    "#     dst_pyr = get_pyramid(rsizer(pyr[0]), depth=15, ratio=3/4, verbose=False)\n",
    "#     for k in range(2):\n",
    "#         start = time.time()\n",
    "#         out = new_image_generation_old(pyr, dst_pyr)\n",
    "#         print('ratio=%s' % (ratio,), 'k=%d' % (k,), 'time=%.2fms' % (1000 * (time.time() - start),))\n",
    "#         imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PatchNNL2(nn.Module):\n",
    "#     def __init__(self, patch_size, reduce='mean', use_padding=False, faiss_device=None):\n",
    "#         super().__init__()\n",
    "#         self._patch_size = _pair(patch_size)\n",
    "#         self._reduce = reduce\n",
    "#         self._use_padding = use_padding\n",
    "#         self._faiss_device = faiss_device\n",
    "    \n",
    "#     @torch.no_grad()\n",
    "#     def forward(self, query, key, value):\n",
    "#         query = unfold2d(query, self._patch_size, use_padding=self._use_padding)\n",
    "#         query, size, ndim = view_as_2d(query)\n",
    "#         key, value = self.extract_kvs(key, value)\n",
    "\n",
    "#         # perform nn-search\n",
    "#         key_index = self.create_index(key)\n",
    "#         idx = key_index.search(query.contiguous(), 1)[1].squeeze(-1)\n",
    "#         assert idx.dim() == 1\n",
    "#         result = torch.index_select(value, 0, idx)\n",
    "\n",
    "#         # reshape output\n",
    "#         output = view_2d_as(result, size, ndim)\n",
    "#         output = fold2d(output, reduce=self._reduce, use_padding=self._use_padding)\n",
    "#         return output\n",
    "    \n",
    "#     @torch.no_grad()\n",
    "#     def extract_kvs(self, key_images, value_images):\n",
    "#         if isinstance(key_images, torch.Tensor):\n",
    "#             key_images = [key_images]\n",
    "#         if isinstance(value_images, torch.Tensor):\n",
    "#             value_images = [value_images]\n",
    "#         assert len(key_images) == len(value_images)\n",
    "#         assert all(k.shape == v.shape for k, v in zip(key_images, value_images))\n",
    "#         key = torch.cat([view_as_2d(unfold2d(k, self._patch_size, use_padding=self._use_padding))[0] for k in key_images], dim=0)\n",
    "#         value = torch.cat([view_as_2d(unfold2d(v, self._patch_size, use_padding=self._use_padding))[0] for v in value_images], dim=0)\n",
    "#         return key, value\n",
    "    \n",
    "#     @torch.no_grad()\n",
    "#     def create_index(self, key):\n",
    "#         assert key.dim() == 2\n",
    "#         d = key.shape[1]\n",
    "#         if self._faiss_device is not None:\n",
    "#             index = faiss.GpuIndexFlatL2(self._faiss_device, d)\n",
    "#         else:\n",
    "#             index = faiss.IndexFlatL2(d)\n",
    "#         index.add(key)\n",
    "#         return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_key_index(key, faiss_gpu_device):\n",
    "#     assert key.dim() == 2\n",
    "#     d = key.shape[1]\n",
    "#     if faiss_gpu_device is not None:\n",
    "#         key_index = faiss.GpuIndexFlatL2(faiss_gpu_device, d)\n",
    "#     else:\n",
    "#         key_index = faiss.IndexFlatL2(d)\n",
    "#     key_index.add(key)\n",
    "#     return key_index\n",
    "\n",
    "# def extract_kvs(key_images, value_images, patch_size, use_padding=True):\n",
    "#     if isinstance(key_images, torch.Tensor):\n",
    "#         key_images = [key_images]\n",
    "#     if isinstance(value_images, torch.Tensor):\n",
    "#         value_images = [value_images]\n",
    "#     assert len(key_images) == len(value_images)\n",
    "#     assert all(k.shape == v.shape for k, v in zip(key_images, value_images))\n",
    "#     key = torch.cat([view_as_2d(unfold2d(k, patch_size, use_padding=use_padding))[0] for k in key_images], dim=0)\n",
    "#     value = torch.cat([view_as_2d(unfold2d(v, patch_size, use_padding=use_padding))[0] for v in value_images], dim=0)\n",
    "#     return key, value\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def pnn_fn(query, key, value, patch_size, reduce='mean', use_padding=True):\n",
    "#     # reshape input\n",
    "#     query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "#     query, size, ndim = view_as_2d(query)\n",
    "#     key, value = extract_kvs(key, value, patch_size, use_padding=use_padding)\n",
    "    \n",
    "#     # perform nn-search\n",
    "#     key_index = create_key_index(key, faiss_gpu_device)\n",
    "#     idx = key_index.search(query.contiguous(), 1)[1].squeeze(-1)\n",
    "#     assert idx.dim() == 1\n",
    "#     result = torch.index_select(value, 0, idx)\n",
    "    \n",
    "#     # reshape output\n",
    "#     result = view_2d_as(result, size, ndim)\n",
    "#     output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "#     return output\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def preindexed_pnn_fn(query, key_index, value, patch_size, reduce='mean', use_padding=True, device=device, faiss_gpu_device=faiss_gpu_device):\n",
    "#     # reshape input\n",
    "#     query = unfold2d(query, patch_size, use_padding=use_padding)\n",
    "#     query, size, ndim = view_as_2d(query)\n",
    "    \n",
    "#     # perform nn-search\n",
    "#     idx = key_index.search(query.contiguous(), 1)[1].squeeze(-1)\n",
    "#     assert idx.dim() == 1\n",
    "#     result = torch.index_select(value, 0, idx)\n",
    "\n",
    "#     # reshape output\n",
    "#     result = view_2d_as(result, size, ndim)\n",
    "#     output = fold2d(result, reduce=reduce, use_padding=use_padding)\n",
    "#     return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
